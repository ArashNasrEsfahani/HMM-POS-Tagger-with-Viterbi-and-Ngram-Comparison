{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7845c6a0",
      "metadata": {
        "id": "7845c6a0"
      },
      "source": [
        "## NLP Homework 2\n",
        "**Arash Nasr Esfahani**\n",
        "\n",
        "**Student ID: 610303068**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ae39c1",
      "metadata": {},
      "source": [
        "### Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f7c32f7e",
      "metadata": {
        "id": "f7c32f7e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "LOG_ZERO = -1e10\n",
        "\n",
        "def log_prob(p):\n",
        "    if p == 0:\n",
        "        return LOG_ZERO\n",
        "    return math.log(p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcfed48b",
      "metadata": {},
      "source": [
        "### Section 1: Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "pzI0bbAdGauD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzI0bbAdGauD",
        "outputId": "dd2f0d32-c89b-4ff7-dc21-5cc313c49c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2449c2ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2449c2ed",
        "outputId": "824b39d7-84ec-407c-b983-20daae3c28e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of training sentences: 9631\n"
          ]
        }
      ],
      "source": [
        "def load_training_data(train_file):\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "\n",
        "    with open(train_file, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line or line.startswith('#'):\n",
        "                if current_sentence:\n",
        "                    sentences.append(current_sentence)\n",
        "                    current_sentence = []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 2:\n",
        "                word = parts[0]\n",
        "                tag = parts[1]\n",
        "                current_sentence.append((word, tag))\n",
        "        if current_sentence:\n",
        "            sentences.append(current_sentence)\n",
        "    return sentences\n",
        "\n",
        "# Loading training data\n",
        "train_sentences = load_training_data('/content/drive/MyDrive/train.txt')\n",
        "print(\"Total number of training sentences:\", len(train_sentences))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0106912e",
      "metadata": {},
      "source": [
        "### Section 2: Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e96f33d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e96f33d6",
        "outputId": "70e32fa0-d8c7-4540-f831-134f5baa1f49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training sentences: 7704\n",
            "Validation sentences: 1927\n"
          ]
        }
      ],
      "source": [
        "def train_validation_split(sentences, validation_frac=0.2, seed=42):\n",
        "    random.seed(seed)\n",
        "    shuffled = sentences.copy()\n",
        "    random.shuffle(shuffled)\n",
        "    split_index = int(len(shuffled) * (1 - validation_frac))\n",
        "    return shuffled[:split_index], shuffled[split_index:]\n",
        "\n",
        "train_data, validation_data = train_validation_split(train_sentences, validation_frac=0.2)\n",
        "print(\"Training sentences:\", len(train_data))\n",
        "print(\"Validation sentences:\", len(validation_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "165a1f88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "165a1f88",
        "outputId": "aba15a87-409e-45b0-d4fd-2dfad57bcd9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of unique tags: 40\n",
            "Vocabulary size: 66136\n"
          ]
        }
      ],
      "source": [
        "tag_counts = Counter()\n",
        "emission_counts = defaultdict(Counter)\n",
        "bigram_counts = defaultdict(Counter)\n",
        "trigram_counts = defaultdict(Counter)\n",
        "\n",
        "START = \"<s>\"\n",
        "STOP = \"</s>\"\n",
        "\n",
        "# Process training sentences\n",
        "for sentence in train_data:\n",
        "    tags = [START] + [tag for word, tag in sentence] + [STOP]\n",
        "\n",
        "    # Unigram counts\n",
        "    for word, tag in sentence:\n",
        "        tag_counts[tag] += 1\n",
        "        emission_counts[tag][word] += 1\n",
        "\n",
        "    # Bigram counts\n",
        "    for i in range(len(tags) - 1):\n",
        "        bigram_counts[tags[i]][tags[i+1]] += 1\n",
        "\n",
        "    # Trigram counts\n",
        "    tags_tri = [START, START] + [tag for word, tag in sentence] + [STOP]\n",
        "    for i in range(len(tags_tri) - 2):\n",
        "        trigram_counts[(tags_tri[i], tags_tri[i+1])][tags_tri[i+2]] += 1\n",
        "\n",
        "# Vocabulary\n",
        "vocab = set()\n",
        "for sentence in train_data:\n",
        "    for word, tag in sentence:\n",
        "        vocab.add(word)\n",
        "\n",
        "print(\"Number of unique tags:\", len(tag_counts))\n",
        "print(\"Vocabulary size:\", len(vocab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "37e95caf",
      "metadata": {
        "id": "37e95caf"
      },
      "outputs": [],
      "source": [
        "tags_list = list(tag_counts.keys())\n",
        "\n",
        "# For emission probabilities, applying Laplace smoothing.\n",
        "emission_prob = defaultdict(dict)\n",
        "for tag in emission_counts:\n",
        "    total = tag_counts[tag] + len(vocab)\n",
        "    for word in vocab:\n",
        "        emission_prob[tag][word] = (emission_counts[tag][word] + 1) / total\n",
        "    emission_prob[tag]['<UNK>'] = 1 / total\n",
        "\n",
        "# Unigram tag probabilities\n",
        "total_tags = sum(tag_counts.values())\n",
        "unigram_tag_prob = { tag: count/total_tags for tag, count in tag_counts.items() }\n",
        "\n",
        "# Bigram transition probabilities\n",
        "bigram_prob = defaultdict(dict)\n",
        "for prev_tag in bigram_counts:\n",
        "    total = sum(bigram_counts[prev_tag].values())\n",
        "    for curr_tag in bigram_counts[prev_tag]:\n",
        "        bigram_prob[prev_tag][curr_tag] = bigram_counts[prev_tag][curr_tag] / total\n",
        "\n",
        "# Trigram transition probabilities\n",
        "trigram_prob = defaultdict(dict)\n",
        "for prev_tags in trigram_counts:\n",
        "    total = sum(trigram_counts[prev_tags].values())\n",
        "    for curr_tag in trigram_counts[prev_tags]:\n",
        "        trigram_prob[prev_tags][curr_tag] = trigram_counts[prev_tags][curr_tag] / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85b6bfd6",
      "metadata": {},
      "source": [
        "### Section 3: Implementation of N-Gram Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3f98a869",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f98a869",
        "outputId": "c4ec565c-7f6f-4e3f-98bc-e01facdab5df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram tags: ['ADJ_SUP', 'N_SING', 'N_SING']\n"
          ]
        }
      ],
      "source": [
        "def viterbi_unigram(sentence, unigram_tag_prob, emission_prob):\n",
        "    best_tags = []\n",
        "    for word in sentence:\n",
        "        best_score = LOG_ZERO\n",
        "        best_tag = None\n",
        "        for tag in unigram_tag_prob:\n",
        "            emit = emission_prob[tag][word] if word in emission_prob[tag] else emission_prob[tag]['<UNK>']\n",
        "            score = log_prob(unigram_tag_prob[tag]) + log_prob(emit)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_tag = tag\n",
        "        best_tags.append(best_tag)\n",
        "    return best_tags\n",
        "\n",
        "\n",
        "sample_sentence = [\"اولین\", \"سیاره\", \"خارج\"]\n",
        "print(\"Unigram tags:\", viterbi_unigram(sample_sentence, unigram_tag_prob, emission_prob))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "23ba51f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ba51f3",
        "outputId": "484b2ec8-f8a5-444c-8bf2-574ed0d3c67a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram tags: ['ADJ_SUP', 'N_SING', 'N_SING']\n"
          ]
        }
      ],
      "source": [
        "def viterbi_bigram(sentence, bigram_prob, emission_prob):\n",
        "    n = len(sentence)\n",
        "    states = list(unigram_tag_prob.keys())\n",
        "\n",
        "    viterbi = [{} for _ in range(n)]\n",
        "    backpointer = [{} for _ in range(n)]\n",
        "\n",
        "    # Initialization\n",
        "    for s in states:\n",
        "        # emission probability\n",
        "        emit = emission_prob[s][sentence[0]] if sentence[0] in emission_prob[s] else emission_prob[s]['<UNK>']\n",
        "        # Transition probability\n",
        "        trans = bigram_prob[START].get(s, 1e-6)  # small smoothing probability for unseen transitions\n",
        "        viterbi[0][s] = log_prob(trans) + log_prob(emit)\n",
        "        backpointer[0][s] = START\n",
        "\n",
        "    # Recursion\n",
        "    for t in range(1, n):\n",
        "        for s in states:\n",
        "            best_prob = LOG_ZERO\n",
        "            best_state = None\n",
        "            emit = emission_prob[s][sentence[t]] if sentence[t] in emission_prob[s] else emission_prob[s]['<UNK>']\n",
        "            log_emit = log_prob(emit)\n",
        "            for s_prev in states:\n",
        "                trans = bigram_prob[s_prev].get(s, 1e-6)  # smoothing for unseen transitions\n",
        "                prob = viterbi[t-1][s_prev] + log_prob(trans) + log_emit\n",
        "                if prob > best_prob:\n",
        "                    best_prob = prob\n",
        "                    best_state = s_prev\n",
        "            viterbi[t][s] = best_prob\n",
        "            backpointer[t][s] = best_state\n",
        "\n",
        "    # Termination\n",
        "    best_final_prob = LOG_ZERO\n",
        "    best_final_state = None\n",
        "    for s in states:\n",
        "        trans = bigram_prob[s].get(STOP, 1e-6)\n",
        "        prob = viterbi[n-1][s] + log_prob(trans)\n",
        "        if prob > best_final_prob:\n",
        "            best_final_prob = prob\n",
        "            best_final_state = s\n",
        "\n",
        "    # Backtracking:\n",
        "    tags = [None] * n\n",
        "    tags[-1] = best_final_state\n",
        "    for t in range(n-1, 0, -1):\n",
        "        tags[t-1] = backpointer[t][tags[t]]\n",
        "    return tags\n",
        "\n",
        "print(\"Bigram tags:\", viterbi_bigram(sample_sentence, bigram_prob, emission_prob))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "522c80ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "522c80ab",
        "outputId": "c1d750eb-9cfd-446f-8646-cb4f0001e0ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trigram tags: ['ADJ_SUP', 'N_SING', 'N_SING']\n"
          ]
        }
      ],
      "source": [
        "def viterbi_trigram(sentence, trigram_prob, emission_prob):\n",
        "    n = len(sentence)\n",
        "    states = list(unigram_tag_prob.keys())\n",
        "\n",
        "    viterbi = [{} for _ in range(n)]\n",
        "    backpointer = [{} for _ in range(n)]\n",
        "\n",
        "    # Initialization\n",
        "    for s in states:\n",
        "        emit = emission_prob[s][sentence[0]] if sentence[0] in emission_prob[s] else emission_prob[s]['<UNK>']\n",
        "        trans = trigram_prob[(START, START)].get(s, 1e-6)\n",
        "        score = log_prob(trans) + log_prob(emit)\n",
        "        viterbi[0][(START, s)] = score\n",
        "        backpointer[0][(START, s)] = (START, START)\n",
        "\n",
        "    # Recursion\n",
        "    for t in range(1, n):\n",
        "        for (prev1, prev2), score_prev in viterbi[t-1].items():\n",
        "            for s in states:\n",
        "                emit = emission_prob[s][sentence[t]] if sentence[t] in emission_prob[s] else emission_prob[s]['<UNK>']\n",
        "                trans = trigram_prob[(prev1, prev2)].get(s, 1e-6)\n",
        "                score = score_prev + log_prob(trans) + log_prob(emit)\n",
        "                key = (prev2, s)\n",
        "                if key not in viterbi[t] or score > viterbi[t][key]:\n",
        "                    viterbi[t][key] = score\n",
        "                    backpointer[t][key] = (prev1, prev2)\n",
        "\n",
        "    # Termination\n",
        "    best_prob = LOG_ZERO\n",
        "    best_pair = None\n",
        "    final_t = n - 1\n",
        "    for (prev, curr), score in viterbi[final_t].items():\n",
        "        trans = trigram_prob[(prev, curr)].get(STOP, 1e-6)\n",
        "        total_score = score + log_prob(trans)\n",
        "        if total_score > best_prob:\n",
        "            best_prob = total_score\n",
        "            best_pair = (prev, curr)\n",
        "\n",
        "    # Backtracking:\n",
        "    tags = [None] * n\n",
        "    tags[-1] = best_pair[1]\n",
        "    if n >= 2:\n",
        "        tags[-2] = best_pair[0]\n",
        "\n",
        "    for t in range(n-1, 0, -1):\n",
        "        key = best_pair\n",
        "        prev_pair = backpointer[t][key]\n",
        "        best_pair = (prev_pair[0], key[0])\n",
        "        tags[t-1] = best_pair[1]\n",
        "    return tags\n",
        "\n",
        "print(\"Trigram tags:\", viterbi_trigram(sample_sentence, trigram_prob, emission_prob))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9efdd683",
      "metadata": {},
      "source": [
        "### Section 4: Evaluation of N-Gram Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c2ece8c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ece8c0",
        "outputId": "ec1aba37-0b8b-480a-922c-9d20880f5a20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating sentences: 100%|██████████| 1927/1927 [00:13<00:00, 141.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram accuracy on validation set: 0.9489186853800516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating sentences: 100%|██████████| 1927/1927 [04:30<00:00,  7.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram accuracy on validation set: 0.9388429125349479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(data, tagging_func):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for sentence in tqdm(data, desc=\"Evaluating sentences\"):\n",
        "        words = [w for w, t in sentence]\n",
        "        gold_tags = [t for w, t in sentence]\n",
        "        predicted_tags = tagging_func(words)\n",
        "        for gold, pred in zip(gold_tags, predicted_tags):\n",
        "            total += 1\n",
        "            if gold == pred:\n",
        "                correct += 1\n",
        "    return correct / total if total > 0 else 0\n",
        "\n",
        "# Evaluating Unigram\n",
        "acc_uni = evaluate_model(validation_data,\n",
        "                           lambda sent: viterbi_unigram(sent, unigram_tag_prob, emission_prob))\n",
        "print(\"Unigram accuracy on validation set:\", acc_uni)\n",
        "\n",
        "# Evaluating Bigram\n",
        "acc_bi = evaluate_model(validation_data,\n",
        "                        lambda sent: viterbi_bigram(sent, bigram_prob, emission_prob))\n",
        "print(\"Bigram accuracy on validation set:\", acc_bi)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "fVqJb4RZKVXz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVqJb4RZKVXz",
        "outputId": "258ce8a7-10e4-474b-efe4-54347f8417ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating in parallel: 100%|██████████| 1927/1927 [3:14:21<00:00,  6.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parallel evaluation accuracy (trigram): 0.9392919220200239\n"
          ]
        }
      ],
      "source": [
        "def tagging_trigram_wrapper(words):\n",
        "    return viterbi_trigram(words, trigram_prob, emission_prob)\n",
        "\n",
        "def eval_sentence(sentence, tagging_func):\n",
        "    words = [w for w, t in sentence]\n",
        "    gold_tags = [t for w, t in sentence]\n",
        "    predicted_tags = tagging_func(words)\n",
        "    correct = sum(1 for gold, pred in zip(gold_tags, predicted_tags) if gold == pred)\n",
        "    total = len(gold_tags)\n",
        "    return correct, total\n",
        "\n",
        "def evaluate_model_parallel(data, tagging_func, max_workers=None):\n",
        "    total_correct = 0\n",
        "    total_words = 0\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = [executor.submit(eval_sentence, sentence, tagging_func) for sentence in data]\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Evaluating in parallel\"):\n",
        "            correct, total = future.result()\n",
        "            total_correct += correct\n",
        "            total_words += total\n",
        "\n",
        "    return total_correct / total_words if total_words > 0 else 0\n",
        "\n",
        "acc_tri = evaluate_model_parallel(validation_data, tagging_trigram_wrapper)\n",
        "print(\"Parallel evaluation accuracy (trigram):\", acc_tri)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3a3c6da7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a3c6da7",
        "outputId": "a51a0597-e545-4e8a-a425-afcde1186671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model based on validation accuracy: unigram\n"
          ]
        }
      ],
      "source": [
        "accuracies = {'unigram': acc_uni, 'bigram': acc_bi, 'trigram': acc_tri}\n",
        "best_model = max(accuracies, key=accuracies.get)\n",
        "print(\"Best model based on validation accuracy:\", best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "828cb992",
      "metadata": {},
      "source": [
        "### Section 5: POS Tagging Using The Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "961d1b22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "961d1b22",
        "outputId": "98b1dee8-2cbc-486f-9607-76fcb71395a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of test words: 129906\n",
            "Predicted tags have been written to /content/drive/MyDrive/test_tags.txt\n"
          ]
        }
      ],
      "source": [
        "def load_test_data(test_file):\n",
        "    words = []\n",
        "    with open(test_file, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            word = line.strip()\n",
        "            if word:\n",
        "                words.append(word)\n",
        "    return words\n",
        "\n",
        "# Reading the test data\n",
        "test_words = load_test_data('/content/drive/MyDrive/test_notags.txt')\n",
        "print(\"Number of test words:\", len(test_words))\n",
        "\n",
        "\n",
        "model_tagging_functions = {\n",
        "    'unigram': lambda sent: viterbi_unigram(sent, unigram_tag_prob, emission_prob),\n",
        "    'bigram':  lambda sent: viterbi_bigram(sent, bigram_prob, emission_prob),\n",
        "    'trigram': lambda sent: viterbi_trigram(sent, trigram_prob, emission_prob)\n",
        "}\n",
        "\n",
        "# Tagging the test words using the selected best model.\n",
        "selected_tagger = model_tagging_functions[best_model]\n",
        "predicted_tags = selected_tagger(test_words)\n",
        "\n",
        "\n",
        "output_file = '/content/drive/MyDrive/test_tags.txt'\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    for word, tag in zip(test_words, predicted_tags):\n",
        "        f.write(f\"{word}\\t{tag}\\n\")\n",
        "\n",
        "print(\"Predicted tags have been written to\", output_file)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
